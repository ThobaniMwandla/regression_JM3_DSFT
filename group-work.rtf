{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset161 Calibri;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\b\f0\fs24\lang9 Basic linear model\fs22\par
\b0 Linear regression models are used to show or predict the relationship between two\~ variables or factors\fs28 .\par
\b\fs24 What is a basic linear model used for?\par
\b0\fs22 Linear regression models\~are\~used to show or predict the relationship between two variables or factors. .\par
\b\fs24 Advantages of linear basic model\b0\fs22\par
- Linear regression performs exceptionally well for linearly separable data\par
- Easier to implement, interpret and efficient to train\par
- It handles overfitting pretty well using dimensionality reduction techniques, regularization, and cross-validation\par
- One more advantage is the extrapolation beyond a specific data set\par
\par
\b\fs24 Disadvantages\~of Basic linear model\b0\fs22\par
- The assumption of linearity between dependent and independent variables\par
- It is often quite prone to noise and overfitting\par
- Linear regression is quite sensitive to outliers\par
- It is prone to multicollinearity\par
\par
\b\fs24 Ridge regression model\par
\b0\fs22 Ridge regression\~is a\~\~model tuning\~method that is\~\~used to analyze\~any data that suffers from multicollinearity.\par
\b\fs24 What is ridge regression model used for?\par
\b0\fs22 Ridge regression\~is a technique\~used to eliminate multicollinearity in data models\par
\b\fs24\par
Advantages\par
\b0\fs22 - Avoids overfitting a model.\par
- They do not require unbiased estimators.\par
- They\~add\~just enough bias to make the estimates reasonably reliable approximations to true population values.\par
- They still perform well in cases of\~a large multivariate data with the number of predictors (p) larger than the number of observations (n).\par
- The ridge estimator is preferably good at improving the least-squares estimate when there is multicollinearity\b\fs24 .\par
Disadvantages\par
\b0\fs22 -They include all the predictors in the final model.\par
-They are unable to perform feature selection.\par
-They shrink the coefficients towards zero.\par
- They trade the variance for bias\par
\b\fs24 Lasso regression model\par
\b0\fs22 The \ldblquote LASSO\rdblquote  stands for\~Least\~Absolute\~Shrinkage and\~Selection\~Operator. Lasso regression is\~a\~regression\~analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical mode.\par
\b\fs24\par
What is\~ LASSO used for\par
\b0\fs22 It is used\~over\~regression\~methods for a more accurate prediction. This model uses\~shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\par
\par
\b\fs24 Advantages\b0\fs22\par
advantages in using LASSO method, first of all it can provide a very good prediction accuracy, because shrinking and removing the coefficients can reduce variance without a substantial increase of the bias, this is especially useful when you have a small number of observations and a large number of features.\par
LASSO helps to increase the model interpretability by eliminating irrelevant variables that are not associated with the response variable, this way also overfitting is reduced. This is the point where we are more interested in because in this paper the focus is on the feature selection task\~\b\fs24\par
\par
Disadvantage of LASSO:\par
\~\b0\fs22 LASSO\~selects at most n variables before it saturates.\~LASSO\~can not do group selection. If there is a group of variables among which the pairwise correlations are very high, then the\~LASSO\~tends to arbitrarily select only one variable from the group\b\fs24\par
Disadvantage of LASSO:\par
\~\b0\fs22 LASSO\~selects at most n variables before it saturates.\~LASSO\~can not do group selection. If there is a group of variables among which the pairwise correlations are very high, then the\~LASSO\~tends to arbitrarily select only one variable from the groupAdvantagesadvantages in using LASSO method, first of all it can provide a very good prediction accuracy, because shrinking and removing the coefficients can reduce variance without a substantial increase of the bias, this is especially useful when you have a small number of observation and a large number of features.\par
\~LASSO helps to increase the model interpretability by eliminating irrelevant variables that are not associated with the response variable, this way also overfitting is reduced. This is the point where we are more interested in because in this paper the focus is on the feature selection task\f1\fs28\lang1032\par
\b\fs24 Decision \f0\lang7177 T\f1\lang1032 ree\f0\lang7177 s\b0\f1\fs28\lang1032\par
\fs22 Decision Trees\~are a non-parametric supervised learning\~method used for both classification and regression tasks. The goal is to create a model\~that predicts the value of a target variable by learning\~simple decision\~rules inferred from the data features.\fs28\par
\b\fs24 What are decision trees used for\fs28\par
\b0\f0\fs22\lang7177 -\f1\lang1032 Decision trees\~are\~used\~for handling non-linear data sets effective\f0\lang7177 ly\f1\fs28\lang1032\par
\b\fs24 Advantages:\b0\par
\f0\fs22\lang7177 - \f1\lang1032 Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\par
\f0\lang7177 - \f1\lang1032 A decision tree does not require normalization of data.\par
\f0\lang7177 - \f1\lang1032 A decision tree does not require scaling of data as well.\par
\f0\lang7177 - \f1\lang1032 Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.\par
\f0\lang7177 - \f1\lang1032 A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders.\par
\b\f0\lang7177 Disadvantages:\b0\f1\fs28\lang1032\par
\f0\fs22\lang7177 - A small change in the data can cause a large change in the structure of the decision tree causing instability.\par
- For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\par
- Decision tree often involves higher time to train the model.\par
- Decision tree training is relatively expensive as the complexity and time has taken are more.\par
- The Decision Tree algorithm is inadequate for applying regression and predicting continuous values.\b\par
\fs28\par
\fs24 Extreme Gradient boosting \fs28\par
\b0\fs22 XGBoost stands for Extreme Gradient Boosting;\par
 it is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best tree model.\par
\fs28\par
\b\fs24 What is it used for\b0\fs28\par
- \fs22 XGBoost is a library used  for developing  fast and high performance gradient boosting tree models. \b\fs28\par
\fs24\par
Advantages:\par
\b0\fs28 - \fs22 Often provides predictive accuracy that cannot be trumped.\par
- Lots of flexibility - can optimize on different loss functions and provides -  several hyper parameter tuning options that make the function fit very flexible.\par
- No data pre-processing required - often works great with categorical and  numerical values as is.\par
- Handles missing data - imputation not required.\par
\fs28\par
\b\fs22 Disadvantages\fs28\par
\b0\fs22 - Gradient Boosting Models will continue improving to minimize all errors.  This can overemphasize outliers and cause overfitting.\par
- Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive.\par
- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.\par
- Less interpretative in nature, although this is easily addressed with various tools.\fs28\par
\b\fs24 Random Forest\fs28\par
\b0\fs22 Random forest is a technique used in modeling predictions and behavior analysis and is built on decision trees.  It contains many decision trees that represent a distinct instance of the classification of data input into the random forest. \b\fs28\par
\fs24 What is it used for\fs28\par
\b0\fs22 Random forest algorithm  can be used for both classifications and regression task. It provides higher accuracy through cross validation.\par
 Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data.\b\par
\fs24 Advantages\fs28\par
\b0\fs22 - It reduces overfitting in decision trees and helps to improve the accuracy\par
- It is flexible to both classification and regression problems\par
- It works well with both categorical and continuous values\par
- It automates missing values present in the data\par
- Normalising of data is not required as it uses a rule-based approach.\par
\b\fs24 Disadvantages\fs28\par
\b0\fs22 - It requires much computational power as well as resources as it builds numerous trees to combine their outputs. \par
- It also requires much time for training as it combines a lot of decision trees to determine the class.\par
- Due to the ensemble of decision trees, it also suffers interpretability and fails to determine the significance of each variable.\par
\b\fs28\par
 \par
\b0\lang9\par
}
 